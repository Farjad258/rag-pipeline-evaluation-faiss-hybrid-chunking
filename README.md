# Evaluating RAG Pipelines with FAISS and Hybrid Retrieval (Fixed vs Sentence Chunking)

This repository contains a **reproducible research implementation** of a Retrieval-Augmented Generation (RAG)
pipeline evaluated on the **20 Newsgroups** dataset. It compares:

- **Retrieval:** FAISS (dense) vs **Hybrid** retrieval (FAISS + BM25)
- **Chunking:** fixed-length vs sentence-based chunking
- **Metrics:** precision, recall, hallucination rate, and answer length

This code accompanies an academic manuscript intended for **IEEE / Springer** submission and is structured
as a paper-companion repository.

---

## Repository Structure

- `notebooks/` — Main executable notebook (end-to-end pipeline)
- `src/` — (Optional) Future refactor into Python modules
- `experiments/` — (Optional) Experiment runner scripts
- `figures/` — Publication-quality plots for the paper
- `data/` — Outputs generated by the notebook (CSV results, embeddings, chunk files)

---

## Method Summary (What the notebook does)

The notebook implements the following pipeline:

1. **Dataset**: loads 20 Newsgroups via scikit-learn (`fetch_20newsgroups`)
2. **Chunking**:
   - Fixed-length chunking
   - Sentence-based chunking (regex-based; no NLTK dependency)
   Chunks are saved under `data/chunks/`
3. **Embeddings**: SentenceTransformer `all-MiniLM-L6-v2` → saved to `data/embeddings.pkl`
4. **Retrieval**:
   - FAISS index (`IndexFlatL2`)
   - BM25 index (`rank_bm25`)
   - Hybrid retrieval = union of FAISS + BM25 top-k results (deduplicated)
5. **Generation**: `transformers` text-generation pipeline using `EleutherAI/gpt-neo-125M`
6. **Evaluation**:
   - overlap-based **precision/recall**
   - **hallucination rate** (tokens not present in retrieved context)
   - **answer length**
   Outputs:
   - `data/rag_evaluation_results.csv`
   - `data/rag_experiment_results.csv`
7. **Plots**: matplotlib/seaborn bar plots for paper-ready visualization

---

## Quickstart (Reproduce Results)

### 1) Install dependencies
```bash
pip install -r requirements.txt

